{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C_EwZ9sfbPkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "126ade47-5e07-4807-cace-4c7caeeb39af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                              title  \\\n",
            "0           0  Sanders back in U.S. Senate, blasts 'coloniali...   \n",
            "1           1  Kremlin: Syria peoples' congress being 'active...   \n",
            "2           2   Oregon Cop Convicted Of Shattering Biker’s Co...   \n",
            "3           3   Twitter Erupts With Glee Over #CruzSexScandal...   \n",
            "4           4  MUST WATCH VIDEO: Obama Tries To Trash Trump B...   \n",
            "\n",
            "                                                text  label  \n",
            "0  WASHINGTON (Reuters) - Democratic U.S. preside...      1  \n",
            "1  MOSCOW (Reuters) - A proposal to convene a con...      1  \n",
            "2  In a baffling fit of rage, an Oregon State Pol...      0  \n",
            "3  The last thing any politician running for the ...      0  \n",
            "4  This is too good to miss! Mr. Teleprompter did...      0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('evaluation.csv', sep=';')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('test (1).csv', sep=';')\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeT1eoTreHR2",
        "outputId": "82ce4a82-e162-4568-e9ea-54e45d25668f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                              title  \\\n",
            "0           0  Live from New York, it's a Trump-Clinton remat...   \n",
            "1           1  Catalan separatists to lose majority in tight ...   \n",
            "2           2  North Carolina governor concedes election to D...   \n",
            "3           3  Draft Senate Iran legislation sets tough new U...   \n",
            "4           4  California governor taps U.S. Representative B...   \n",
            "\n",
            "                                                text  label  \n",
            "0  NEW YORK (Reuters) - Veteran actor and frequen...      1  \n",
            "1  BARCELONA (Reuters) - Catalonia s independence...      1  \n",
            "2  WINSTON-SALEM, N.C. (Reuters) - North Carolina...      1  \n",
            "3  WASHINGTON (Reuters) - Draft legislation respo...      1  \n",
            "4  SACRAMENTO, Calif. (Reuters) - California Gove...      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "with open('evaluation.csv', \"r\", encoding=\"utf-8\") as f:\n",
        "    delimiter = csv.Sniffer().sniff(f.readline()).delimiter\n",
        "\n",
        "eval_df = pd.read_csv('evaluation.csv', delimiter=delimiter)\n",
        "\n",
        "print(f\"Detekovaný oddělovač: '{delimiter}'\")\n",
        "print(eval_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UQlsRk0ezL6",
        "outputId": "b3c0eb9b-ee43-4c6e-b1bc-812fcde2104a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detekovaný oddělovač: ';'\n",
            "   Unnamed: 0                                              title  \\\n",
            "0           0  Sanders back in U.S. Senate, blasts 'coloniali...   \n",
            "1           1  Kremlin: Syria peoples' congress being 'active...   \n",
            "2           2   Oregon Cop Convicted Of Shattering Biker’s Co...   \n",
            "3           3   Twitter Erupts With Glee Over #CruzSexScandal...   \n",
            "4           4  MUST WATCH VIDEO: Obama Tries To Trash Trump B...   \n",
            "\n",
            "                                                text  label  \n",
            "0  WASHINGTON (Reuters) - Democratic U.S. preside...      1  \n",
            "1  MOSCOW (Reuters) - A proposal to convene a con...      1  \n",
            "2  In a baffling fit of rage, an Oregon State Pol...      0  \n",
            "3  The last thing any politician running for the ...      0  \n",
            "4  This is too good to miss! Mr. Teleprompter did...      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "with open('test (1).csv', \"r\", encoding=\"utf-8\") as f:\n",
        "    delimiter = csv.Sniffer().sniff(f.readline()).delimiter\n",
        "\n",
        "test_df = pd.read_csv('test (1).csv', delimiter=delimiter)\n",
        "\n",
        "print(f\"Detekovaný oddělovač: '{delimiter}'\")\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJCrR7G4e0PA",
        "outputId": "08523d42-4f76-444c-9ae4-19e7f3c518a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detekovaný oddělovač: ';'\n",
            "   Unnamed: 0                                              title  \\\n",
            "0           0  Live from New York, it's a Trump-Clinton remat...   \n",
            "1           1  Catalan separatists to lose majority in tight ...   \n",
            "2           2  North Carolina governor concedes election to D...   \n",
            "3           3  Draft Senate Iran legislation sets tough new U...   \n",
            "4           4  California governor taps U.S. Representative B...   \n",
            "\n",
            "                                                text  label  \n",
            "0  NEW YORK (Reuters) - Veteran actor and frequen...      1  \n",
            "1  BARCELONA (Reuters) - Catalonia s independence...      1  \n",
            "2  WINSTON-SALEM, N.C. (Reuters) - North Carolina...      1  \n",
            "3  WASHINGTON (Reuters) - Draft legislation respo...      1  \n",
            "4  SACRAMENTO, Calif. (Reuters) - California Gove...      1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "with open('train (2).csv', \"r\", encoding=\"utf-8\") as f:\n",
        "    delimiter = csv.Sniffer().sniff(f.readline()).delimiter\n",
        "\n",
        "train_df = pd.read_csv('train (2).csv', delimiter=delimiter)\n",
        "\n",
        "print(f\"Detekovaný oddělovač: '{delimiter}'\")\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V4sYYvEfGjO",
        "outputId": "cbf99ad9-f637-4bd8-9df2-52bb9da3493a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detekovaný oddělovač: ';'\n",
            "   Unnamed: 0                                              title  \\\n",
            "0           0  Palestinians switch off Christmas lights in Be...   \n",
            "1           1  China says Trump call with Taiwan president wo...   \n",
            "2           2   FAIL! The Trump Organization’s Credit Score W...   \n",
            "3           3  Zimbabwe military chief's China trip was norma...   \n",
            "4           4  THE MOST UNCOURAGEOUS PRESIDENT EVER Receives ...   \n",
            "\n",
            "                                                text  label  \n",
            "0  RAMALLAH, West Bank (Reuters) - Palestinians s...      1  \n",
            "1  BEIJING (Reuters) - U.S. President-elect Donal...      1  \n",
            "2  While the controversy over Trump s personal ta...      0  \n",
            "3  BEIJING (Reuters) - A trip to Beijing last wee...      1  \n",
            "4  There has never been a more UNCOURAGEOUS perso...      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Text Preprocessing and Label Encoding\n",
        "# Here we only need 'title', 'text', and 'label' columns\n",
        "train_df['content'] = train_df['title'] + \" \" + train_df['text']  # Combine title and text\n",
        "eval_df['content'] = eval_df['title'] + \" \" + eval_df['text']\n",
        "test_df['content'] = test_df['title'] + \" \" + test_df['text']\n",
        "\n",
        "# Encode the labels (1 for fake news, 0 otherwise)\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['label'] = label_encoder.fit_transform(train_df['label'])\n",
        "eval_df['label'] = label_encoder.transform(eval_df['label'])\n",
        "\n",
        "# Step 3: Train-Test Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_df['content'], train_df['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Vectorization using TF-IDF and Model Pipeline\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
        "\n",
        "# Create a pipeline with TF-IDF and Logistic Regression\n",
        "model_pipeline = Pipeline([\n",
        "    ('tfidf', tfidf_vectorizer),\n",
        "    ('clf', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Step 5: Model Training\n",
        "model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluation on Validation Set\n",
        "y_pred_val = model_pipeline.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred_val)\n",
        "report = classification_report(y_val, y_pred_val)\n",
        "conf_matrix = confusion_matrix(y_val, y_pred_val)\n",
        "\n",
        "print(\"Validation Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", report)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Step 7: Evaluation on Test Set\n",
        "if 'label' in test_df.columns:\n",
        "    y_test = label_encoder.transform(test_df['label'])\n",
        "    y_pred_test = model_pipeline.predict(test_df['content'])\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "    test_report = classification_report(y_test, y_pred_test)\n",
        "    test_conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "    print(\"Test Accuracy:\", test_accuracy)\n",
        "    print(\"\\nTest Classification Report:\\n\", test_report)\n",
        "    print(\"\\nTest Confusion Matrix:\\n\", test_conf_matrix)\n",
        "else:\n",
        "    # If no labels are provided in test data, output predictions\n",
        "    test_predictions = model_pipeline.predict(test_df['content'])\n",
        "    test_df['predicted_label'] = label_encoder.inverse_transform(test_predictions)\n",
        "    test_df.to_csv('/kaggle/working/test_predictions.csv', index=False)\n",
        "    print(\"Predictions for test set saved to test_predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMDVcZqPfS1t",
        "outputId": "1866fcb3-208e-4e55-9f22-d80adb757111"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9667419421063437\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96      2215\n",
            "           1       0.97      0.97      0.97      2656\n",
            "\n",
            "    accuracy                           0.97      4871\n",
            "   macro avg       0.97      0.97      0.97      4871\n",
            "weighted avg       0.97      0.97      0.97      4871\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[2140   75]\n",
            " [  87 2569]]\n",
            "Test Accuracy: 0.9721572009363065\n",
            "\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97      3753\n",
            "           1       0.97      0.98      0.97      4364\n",
            "\n",
            "    accuracy                           0.97      8117\n",
            "   macro avg       0.97      0.97      0.97      8117\n",
            "weighted avg       0.97      0.97      0.97      8117\n",
            "\n",
            "\n",
            "Test Confusion Matrix:\n",
            " [[3634  119]\n",
            " [ 107 4257]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 📌 1️⃣ **Načtení datasetů**\n",
        "test_df  = pd.read_csv('test (1).csv', sep=';')\n",
        "train_df  = pd.read_csv('train (2).csv', sep=';')\n",
        "\n",
        "# 📌 2️⃣ **Předzpracování dat**\n",
        "# Předpokládám, že poslední sloupec je cílová proměnná (y)\n",
        "target_column = train_df.columns[-1]  # Označení cílového sloupce\n",
        "X_train = train_df.drop(columns=[target_column])\n",
        "y_train = train_df[target_column]\n",
        "\n",
        "X_test = test_df.drop(columns=[target_column])\n",
        "y_test = test_df[target_column]\n",
        "\n",
        "# 📌 **Použití stejného LabelEncoder pro trénovací i testovací data**\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Nejdříve naučíme encoder na y_train\n",
        "encoder.fit(y_train)\n",
        "\n",
        "# Pak převedeme obě sady dat\n",
        "y_train = encoder.transform(y_train)\n",
        "y_test = encoder.transform(y_test)  # Používáme stejný encoder, aby nebyla chyba\n",
        "\n",
        "# Převod na one-hot encoding (pro neuronovou síť)\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Zpracování textových sloupců\n",
        "text_columns = X_train.select_dtypes(include=['object']).columns\n",
        "for col in text_columns:\n",
        "    le = LabelEncoder()\n",
        "    X_train[col] = le.fit_transform(X_train[col])\n",
        "    X_test[col] = le.transform(X_test[col])\n",
        "\n",
        "# Normalizace číselných dat\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 📌 3️⃣ **Vytvoření neuronové sítě**\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Vstupní vrstva\n",
        "    Dropout(0.3),  # Dropout pro snížení přeučení\n",
        "    Dense(64, activation='relu'),  # Skrytá vrstva\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),  # Další skrytá vrstva\n",
        "    Dense(y_train.shape[1], activation='softmax')  # Výstupní vrstva (softmax pro více tříd)\n",
        "])\n",
        "\n",
        "# 📌 4️⃣ **Kompilace modelu**\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 📌 5️⃣ **Trénování modelu**\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# 📌 6️⃣ **Vyhodnocení modelu**\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Převedení pravděpodobností na třídy\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Zobrazení klasifikačního reportu\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "# Zobrazení matice záměn\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_classes, y_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "VyvEWWMWnqFk",
        "outputId": "43c1f3b0-69e2-4c36-c3b5-7296608d5fdf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "y contains previously unseen labels: \"Live from New York, it's a Trump-Clinton rematch - of sorts\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nandict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Live from New York, it's a Trump-Clinton rematch - of sorts\"",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5d22a4e9a5a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Normalizace číselných dat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_map_to_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y contains previously unseen labels: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_unknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: \"Live from New York, it's a Trump-Clinton rematch - of sorts\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# data sets\n",
        "test_df = pd.read_csv('test (1).csv', sep=';')\n",
        "train_df = pd.read_csv('train (2).csv', sep=';')\n",
        "\n",
        "# processing data\n",
        "target_column = train_df.columns[-1]  #target column\n",
        "X_train = train_df.drop(columns=[target_column])\n",
        "y_train = train_df[target_column]\n",
        "\n",
        "X_test = test_df.drop(columns=[target_column])\n",
        "y_test = test_df[target_column]\n",
        "\n",
        "# use same LabelEncoder for testing and training data\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# learning encoder on y_train\n",
        "encoder.fit(y_train)\n",
        "\n",
        "# transform both data sets\n",
        "y_train = encoder.transform(y_train)\n",
        "y_test = encoder.transform(y_test)  # Používáme stejný encoder, aby nebyla chyba\n",
        "\n",
        "# transfer to  one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "# processing text columns\n",
        "text_columns = X_train.select_dtypes(include=['object']).columns\n",
        "for col in text_columns:\n",
        "    le = LabelEncoder()\n",
        "    # Fit the LabelEncoder on the combined unique values from both train and test\n",
        "    all_values = pd.concat([X_train[col], X_test[col]]).unique()\n",
        "    le.fit(all_values)\n",
        "    X_train[col] = le.transform(X_train[col])\n",
        "    X_test[col] = le.transform(X_test[col])\n",
        "\n",
        "# normalize numbers columns\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# neural network creation\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\"\n",
        "    Dropout(0.3),  # Dropout to reduce overfitting\n",
        "    Dense(64, activation='relu'),  # Hidden layer\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),  # Hidden layer\n",
        "    Dense(y_train.shape[1], activation='softmax')  # Output layer (softmax for multiple classes)\n",
        "])\n",
        "\n",
        "# Model compilation\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# training model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# model evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  #converting probabilities to classes\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# displaying the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "# displaying the confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_classes, y_pred_classes))"
      ],
      "metadata": {
        "id": "B8eeO0nmofFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt\n"
      ],
      "metadata": {
        "id": "o8ts8Qc4AXCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_curve, auc\n",
        "\n",
        "# ✅ Cesta k datům (Uprav dle potřeby)\n",
        "data_path = \"C:\\\\Users\\\\fake_news\\\\\"\n",
        "\n",
        "# ✅ Načtení datasetů\n",
        "train_df = pd.read_csv(data_path + \"train.csv\", sep=';')\n",
        "test_df = pd.read_csv(data_path + \"test.csv\", sep=';')\n",
        "submit_df = pd.read_csv(data_path + \"submit.csv\", sep=';')\n",
        "\n",
        "# ✅ Spojení testovacích dat\n",
        "test_df = pd.concat([test_df, submit_df], axis=1).drop(\"idS\", axis=1)\n",
        "df_all = pd.concat([train_df, test_df])\n",
        "\n",
        "\n",
        "# ✅ Rozdělení na vstupní (X) a výstupní (Y) data\n",
        "target_column = df_all.columns[-1]\n",
        "X = df_all.drop(columns=[target_column])\n",
        "Y = df_all[target_column]\n",
        "\n",
        "# ✅ Label Encoding pro výstupní data\n",
        "encoder = LabelEncoder()\n",
        "Y = encoder.fit_transform(Y)\n",
        "Y = keras.utils.to_categorical(Y)  # Převod na one-hot encoding\n",
        "\n",
        "# ✅ Zpracování textových sloupců\n",
        "text_columns = X.select_dtypes(include=['object']).columns\n",
        "for col in text_columns:\n",
        "    le = LabelEncoder()\n",
        "    all_values = pd.concat([X[col], X[col]]).unique()  # Sloučení hodnot z trénovacích a testovacích dat\n",
        "    le.fit(all_values)\n",
        "    X[col] = le.transform(X[col])\n",
        "\n",
        "# ✅ Normalizace číselných sloupců\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# ✅ Tokenizace textových dat (pokud je sloupec `title`)\n",
        "if 'title' in df_all.columns:\n",
        "    max_features = 5000\n",
        "    max_len = 40\n",
        "    tokenizer = Tokenizer(num_words=max_features)\n",
        "    X_title = df_all['title'].astype(str).fillna(\"\")\n",
        "    tokenizer.fit_on_texts(X_title)\n",
        "    sequences = tokenizer.texts_to_sequences(X_title)\n",
        "    X_title = pad_sequences(sequences, maxlen=max_len, padding=\"pre\")\n",
        "else:\n",
        "    X_title = None\n",
        "\n",
        "# ✅ Rozdělení na trénovací a testovací sady\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ✅ Vytvoření neuronové sítě\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Vstupní vrstva\n",
        "    Dropout(0.3),  # Dropout pro snížení přeučení\n",
        "    Dense(64, activation='relu'),  # Skrytá vrstva\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),  # Skrytá vrstva\n",
        "    Dense(y_train.shape[1], activation='softmax')  # Výstupní vrstva (softmax pro více tříd)\n",
        "])\n",
        "\n",
        "# ✅ Kompilace modelu\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ✅ Trénování modelu\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# ✅ Zobrazení průběhu trénování\n",
        "history_dict = history.history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_dict[\"loss\"], \"ro\", label=\"Training loss\")\n",
        "plt.plot(history_dict[\"val_loss\"], \"r\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_dict[\"accuracy\"], \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(history_dict[\"val_accuracy\"], \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# ✅ Predikce na testovacích datech\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Převod pravděpodobností na třídy\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# ✅ Zobrazení klasifikačního reportu\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "# ✅ Zobrazení matice záměn\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_classes, y_pred_classes))\n",
        "\n",
        "# ✅ ROC křivka a AUC skóre\n",
        "y_pred_proba = y_pred.ravel()\n",
        "fpr, tpr, thresholds = roc_curve(y_test_classes, y_pred_proba)\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC (AUC = {auc_score:.3f})')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ✅ Vyhodnocení modelu\n",
        "results = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nModel Loss: {results[0]}, Accuracy: {results[1]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dTH2urJ_BzRo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}