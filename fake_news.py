# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1caqXEaF7OLErMoFFj778sGz55fZrFTCG
"""

df = pd.read_csv('test (1).csv', sep=';')
print(df.head())

import pandas as pd
import csv

with open('evaluation.csv', "r", encoding="utf-8") as f:
    delimiter = csv.Sniffer().sniff(f.readline()).delimiter

eval_df = pd.read_csv('evaluation.csv', delimiter=delimiter)

print(f"Detekovaný oddělovač: '{delimiter}'")
print(eval_df.head())

import pandas as pd
import csv

with open('train (2).csv', "r", encoding="utf-8") as f:
    delimiter = csv.Sniffer().sniff(f.readline()).delimiter

train_df = pd.read_csv('train (2).csv', delimiter=delimiter)

print(f"Detekovaný oddělovač: '{delimiter}'")
print(train_df.head())

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder



# Step 2: Text Preprocessing and Label Encoding
# Here we only need 'title', 'text', and 'label' columns
train_df['content'] = train_df['title'] + " " + train_df['text']  # Combine title and text
eval_df['content'] = eval_df['title'] + " " + eval_df['text']
test_df['content'] = test_df['title'] + " " + test_df['text']

# Encode the labels (1 for fake news, 0 otherwise)
label_encoder = LabelEncoder()
train_df['label'] = label_encoder.fit_transform(train_df['label'])
eval_df['label'] = label_encoder.transform(eval_df['label'])

# Step 3: Train-Test Split
X_train, X_val, y_train, y_val = train_test_split(
    train_df['content'], train_df['label'], test_size=0.2, random_state=42
)

# Step 4: Vectorization using TF-IDF and Model Pipeline
tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')

# Create a pipeline with TF-IDF and Logistic Regression
model_pipeline = Pipeline([
    ('tfidf', tfidf_vectorizer),
    ('clf', LogisticRegression(max_iter=1000))
])

# Step 5: Model Training
model_pipeline.fit(X_train, y_train)

# Step 6: Evaluation on Validation Set
y_pred_val = model_pipeline.predict(X_val)
accuracy = accuracy_score(y_val, y_pred_val)
report = classification_report(y_val, y_pred_val)
conf_matrix = confusion_matrix(y_val, y_pred_val)

print("Validation Accuracy:", accuracy)
print("\nClassification Report:\n", report)
print("\nConfusion Matrix:\n", conf_matrix)

# Step 7: Evaluation on Test Set
if 'label' in test_df.columns:
    y_test = label_encoder.transform(test_df['label'])
    y_pred_test = model_pipeline.predict(test_df['content'])
    test_accuracy = accuracy_score(y_test, y_pred_test)
    test_report = classification_report(y_test, y_pred_test)
    test_conf_matrix = confusion_matrix(y_test, y_pred_test)

    print("Test Accuracy:", test_accuracy)
    print("\nTest Classification Report:\n", test_report)
    print("\nTest Confusion Matrix:\n", test_conf_matrix)
else:
    # If no labels are provided in test data, output predictions
    test_predictions = model_pipeline.predict(test_df['content'])
    test_df['predicted_label'] = label_encoder.inverse_transform(test_predictions)
    test_df.to_csv('/kaggle/working/test_predictions.csv', index=False)
    print("Predictions for test set saved to test_predictions.csv")

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# data sets
test_df = pd.read_csv('test (1).csv', sep=';')
train_df = pd.read_csv('train (2).csv', sep=';')

# processing data
target_column = train_df.columns[-1]  #target column
X_train = train_df.drop(columns=[target_column])
y_train = train_df[target_column]

X_test = test_df.drop(columns=[target_column])
y_test = test_df[target_column]

# use same LabelEncoder for testing and training data
encoder = LabelEncoder()

# learning encoder on y_train
encoder.fit(y_train)

# transform both data sets
y_train = encoder.transform(y_train)
y_test = encoder.transform(y_test)  # same encoder fo no errors

# transfer to  one-hot encoding
y_train = keras.utils.to_categorical(y_train)
y_test = keras.utils.to_categorical(y_test)

# processing text columns
text_columns = X_train.select_dtypes(include=['object']).columns
for col in text_columns:
    le = LabelEncoder()
    # Fit the LabelEncoder on the combined unique values from both train and test
    all_values = pd.concat([X_train[col], X_test[col]]).unique()
    le.fit(all_values)
    X_train[col] = le.transform(X_train[col])
    X_test[col] = le.transform(X_test[col])

# normalize numbers columns
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# neural network creation
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer"
    Dropout(0.3),  # Dropout to reduce overfitting
    Dense(64, activation='relu'),  # Hidden layer
    Dropout(0.3),
    Dense(32, activation='relu'),  # Hidden layer
    Dense(y_train.shape[1], activation='softmax')  # Output layer (softmax for multiple classes)
])

# Model compilation
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# training model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# model evaluation
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  #converting probabilities to classes
y_test_classes = np.argmax(y_test, axis=1)

# displaying the classification report
print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes))

# displaying the confusion matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_classes, y_pred_classes))

# ✅ Importy knihoven
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_curve, auc



# ✅ Načtení datasetů
train_df = pd.read_csv('train (2).csv', sep=';')
test_df = pd.read_csv('test (1).csv', sep=';')
submit_df = pd.read_csv('evaluation.csv', sep=';')

# ✅ Spojení testovacích dat (správné spojení po řádcích)
test_df = pd.concat([test_df, submit_df], axis=0, ignore_index=True).drop(columns=["idS"], errors='ignore')
df_all = pd.concat([train_df, test_df], axis=0, ignore_index=True)

# ✅ Rozdělení na vstupní (X) a výstupní (Y) data
target_column = df_all.columns[-1]  # Poslední sloupec jako cíl
X = df_all.drop(columns=[target_column])
Y = df_all[target_column]

# ✅ Label Encoding pro výstupní data
encoder = LabelEncoder()
Y = encoder.fit_transform(Y)
Y = tf.keras.utils.to_categorical(Y)  # One-hot encoding pro více tříd

# ✅ Zpracování textových sloupců pomocí Label Encodingu
text_columns = X.select_dtypes(include=['object']).columns
for col in text_columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str).fillna(""))

# ✅ Normalizace číselných sloupců
scaler = StandardScaler()
X = scaler.fit_transform(X)

# ✅ Tokenizace textových dat (pokud existuje sloupec `title`)
max_features = 5000  # Maximální počet slov v tokenizéru
max_len = 40  # Maximální délka sekvencí

if 'title' in df_all.columns:
    tokenizer = Tokenizer(num_words=max_features, oov_token="<OOV>")
    X_title = df_all['title'].astype(str).fillna("")
    tokenizer.fit_on_texts(X_title)
    sequences = tokenizer.texts_to_sequences(X_title)
    X_title = pad_sequences(sequences, maxlen=max_len, padding="pre")

    # ✅ Sloučení tokenizovaného textu s X
    X = np.hstack([X, X_title])

# ✅ Rozdělení na trénovací a testovací sady
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ✅ Vytvoření neuronové sítě
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Vstupní vrstva
    Dropout(0.3),  # Dropout pro snížení přeučení
    Dense(64, activation='relu'),  # Skrytá vrstva
    Dropout(0.3),
    Dense(32, activation='relu'),  # Skrytá vrstva
    Dense(y_train.shape[1], activation='softmax')  # Výstupní vrstva (softmax pro více tříd)
])

# ✅ Kompilace modelu
model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ✅ Trénování modelu
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# ✅ Zobrazení průběhu trénování
history_dict = history.history
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history_dict["loss"], "ro", label="Training loss")
plt.plot(history_dict["val_loss"], "r", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(history_dict["accuracy"], "bo", label="Training accuracy")
plt.plot(history_dict["val_accuracy"], "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()

plt.show()

# ✅ Predikce na testovacích datech
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Převod pravděpodobností na třídy
y_test_classes = np.argmax(y_test, axis=1)

# ✅ Zobrazení klasifikačního reportu
print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes))

# ✅ Zobrazení matice záměn
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_classes, y_pred_classes))

# ✅ ROC křivka a AUC skóre
plt.figure(figsize=(6, 6))

if y_train.shape[1] == 2:  # Binary classification
    y_pred_proba = y_pred[:, 1]  # Pravděpodobnosti pro pozitivní třídu
    fpr, tpr, _ = roc_curve(y_test_classes, y_pred_proba)
    auc_score = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'ROC (AUC = {auc_score:.3f})')

else:  # Multiclass classification (One-vs-Rest ROC)
    for i in range(y_train.shape[1]):
        fpr, tpr, _ = roc_curve(y_test[:, i], y_pred[:, i])
        auc_score = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {auc_score:.3f})')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC Curve")
plt.legend()
plt.grid()
plt.show()

# ✅ Vyhodnocení modelu
results = model.evaluate(X_test, y_test)
print(f"\nModel Loss: {results[0]}, Accuracy: {results[1]}")

# ✅ Importy knihoven
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

# ✅ Načtení datasetů
train_df = pd.read_csv('train (2).csv', sep=';')
test_df = pd.read_csv('test (1).csv', sep=';')
submit_df = pd.read_csv('evaluation.csv', sep=';')

# ✅ Spojení testovacích dat
test_df = pd.concat([test_df, submit_df], axis=0, ignore_index=True).drop(columns=["idS"], errors='ignore')
df_all = pd.concat([train_df, test_df], axis=0, ignore_index=True)

# ✅ Rozdělení na vstupní (X) a výstupní (Y) data
target_column = df_all.columns[-1]
X = df_all.drop(columns=[target_column])
Y = df_all[target_column]

# ✅ One-Hot Encoding pro výstupní data (opravená verze)
encoder = OneHotEncoder(sparse_output=False)
Y = encoder.fit_transform(Y.values.reshape(-1, 1))

# ✅ Normalizace číselných sloupců
scaler = StandardScaler()
X_num = scaler.fit_transform(X.select_dtypes(include=[np.number]))

# ✅ Tokenizace textových dat
max_features = 4000 # Více slov pro lepší tokenizaci
max_len = 30  # Delší sekvence
if 'title' in df_all.columns:
    tokenizer = Tokenizer(num_words=max_features, oov_token="<OOV>")
    X_title = df_all['title'].astype(str).fillna("")
    tokenizer.fit_on_texts(X_title)
    sequences = tokenizer.texts_to_sequences(X_title)
    X_title = pad_sequences(sequences, maxlen=max_len, padding="pre")
else:
    X_title = np.zeros((X.shape[0], max_len))

# ✅ Sloučení číselných a textových dat
X = np.hstack([X_num, X_title])

# ✅ Rozdělení na trénovací a testovací sady
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ✅ Callbacks (EarlyStopping + ReduceLROnPlateau)
early_stopping = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, min_lr=1e-6)

# ✅ Vytvoření neuronové sítě s lepší architekturou
model = Sequential([
    Dense(256, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    BatchNormalization(),
    Dropout(0.4),

    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    BatchNormalization(),
    Dropout(0.4),

    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    BatchNormalization(),
    Dropout(0.3),

    Dense(y_train.shape[1], activation='softmax')
])

# ✅ Kompilace modelu
model.compile(optimizer=Adam(learning_rate=5e-4), loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ✅ Trénování modelu
history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test),
                    callbacks=[early_stopping, reduce_lr])

# ✅ Zobrazení průběhu trénování
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history["loss"], "ro-", label="Training loss")
plt.plot(history.history["val_loss"], "r-", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(history.history["accuracy"], "bo-", label="Training accuracy")
plt.plot(history.history["val_accuracy"], "b-", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()

plt.show()

# ✅ Predikce na testovacích datech
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = np.argmax(y_test, axis=1)

# ✅ Klasifikační report a matice záměn
print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test_classes, y_pred_classes))

# ✅ ROC křivka a AUC skóre
plt.figure(figsize=(6, 6))
for i in range(y_train.shape[1]):
    fpr, tpr, _ = roc_curve(y_test[:, i], y_pred[:, i])
    auc_score = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {auc_score:.3f})')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC Curve")
plt.legend()
plt.grid()
plt.show()

# ✅ Importy knihoven
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

# ✅ Načtení datasetů
train_df = pd.read_csv('train (2).csv', sep=';')
test_df = pd.read_csv('test (1).csv', sep=';')
submit_df = pd.read_csv('evaluation.csv', sep=';')

# ✅ Spojení testovacích dat
test_df = pd.concat([test_df, submit_df], axis=0, ignore_index=True).drop(columns=["idS"], errors='ignore')
df_all = pd.concat([train_df, test_df], axis=0, ignore_index=True)

# ✅ Odstranění chybějících hodnot
df_all = df_all.dropna()

# ✅ Rozdělení na vstupní (X) a výstupní (Y) data
target_column = df_all.columns[-1]
X = df_all.drop(columns=[target_column])
Y = df_all[target_column]

# ✅ Label Encoding + OneHotEncoder pro výstupní data (správná verze)
label_encoder = LabelEncoder()
Y_encoded = label_encoder.fit_transform(Y)

one_hot_encoder = OneHotEncoder(sparse_output=False)
Y = one_hot_encoder.fit_transform(Y_encoded.reshape(-1, 1))

# ✅ Normalizace číselných sloupců
scaler = StandardScaler()
X_num = scaler.fit_transform(X.select_dtypes(include=[np.number]))

# ✅ Lepší tokenizace textových dat
max_features = 10000  # Zvýšení slovní zásoby
max_len = 20  # Delší sekvence pro textová data
if 'title' in df_all.columns:
    tokenizer = Tokenizer(num_words=max_features, oov_token="<OOV>")
    X_title = df_all['title'].astype(str).str.lower().fillna("")  # Převedení na malá písmena
    tokenizer.fit_on_texts(X_title)
    sequences = tokenizer.texts_to_sequences(X_title)
    X_title = pad_sequences(sequences, maxlen=max_len, padding="pre")
else:
    X_title = np.zeros((X.shape[0], max_len))

# ✅ Sloučení číselných a textových dat
X = np.hstack([X_num, X_title])

# ✅ Rozdělení na trénovací a testovací sady
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ✅ Callbacks (EarlyStopping + ReduceLROnPlateau)
early_stopping = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, min_lr=1e-6)

# ✅ Vylepšená neuronová síť
model = Sequential([
    Dense(512, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.0005)),
    BatchNormalization(),
    Dropout(0.5),

    Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005)),
    BatchNormalization(),
    Dropout(0.4),

    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005)),
    BatchNormalization(),
    Dropout(0.3),

    Dense(y_train.shape[1], activation='softmax')
])

# ✅ Kompilace modelu
model.compile(optimizer=Adam(learning_rate=3e-4), loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ✅ Trénování modelu
history = model.fit(X_train, y_train, epochs=40, batch_size=128, validation_data=(X_test, y_test),
                    callbacks=[early_stopping, reduce_lr])

# ✅ Zobrazení průběhu trénování
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history["loss"], "ro-", label="Training loss")
plt.plot(history.history["val_loss"], "r-", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(history.history["accuracy"], "bo-", label="Training accuracy")
plt.plot(history.history["val_accuracy"], "b-", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()

plt.show()

# ✅ Predikce na testovacích datech
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = np.argmax(y_test, axis=1)

# ✅ Klasifikační report a matice záměn
print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test_classes, y_pred_classes))

# ✅ ROC křivka a AUC skóre
plt.figure(figsize=(6, 6))
for i in range(y_train.shape[1]):
    fpr, tpr, _ = roc_curve(y_test[:, i], y_pred[:, i])
    auc_score = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {auc_score:.3f})')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC Curve")
plt.legend()
plt.grid()
plt.show()

import matplotlib.pyplot as plt
from collections import Counter
# from keras.preprocessing.text import Tokenizer # This line is causing the error
from tensorflow.keras.preprocessing.text import Tokenizer # Use tensorflow.keras instead
from tensorflow.keras.preprocessing.sequence import pad_sequences # Use tensorflow.keras instead


# ✅ Ujistíme se, že `title` neobsahuje NaN hodnoty
df_all['title'] = df_all['title'].astype(str).fillna("")

# ✅ Tokenizace celého datasetu
max_features = 5000  # Maximální počet slov
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(df_all['title'])

# ✅ Získání četnosti slov
word_counts = dict(tokenizer.word_counts)

# ✅ Seřazení slov podle četnosti
sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)

# ✅ Převedení textu na sekvence tokenů
sequences = tokenizer.texts_to_sequences(df_all['title'])

# ✅ Délky tokenizovaných vět
lengths = [len(seq) for seq in sequences]

# ✅ Histogram délek vět
plt.figure(figsize=(10, 5))
plt.hist(lengths, bins=50, color='blue', alpha=0.7)
plt.xlabel("Délka věty (počet tokenů)")
plt.ylabel("Počet vět")
plt.title("Histogram délek tokenizovaných vět")
plt.grid()
plt.show()

import matplotlib.pyplot as plt
from collections import Counter
# from keras.preprocessing.text import Tokenizer # This was causing the error
from tensorflow.keras.preprocessing.text import Tokenizer # Correct import

# ✅ Ujistíme se, že 'title' neobsahuje NaN hodnoty
df_all['title'] = df_all['title'].astype(str).fillna("")

# ✅ Tokenizace všech textů v datasetu
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df_all['title'])

# ✅ Počet slov v datasetu
word_counts = dict(tokenizer.word_counts)

# ✅ Seřazení slov podle četnosti (sestupně)
sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)

# ✅ Výběr 30 nejčastějších slov
top_words = sorted_word_counts[:30]
words, counts = zip(*top_words)  # Rozdělení na slova a jejich četnosti

# ✅ Graf distribuce četnosti slov
plt.figure(figsize=(12, 6))
plt.bar(words, counts, color='blue', alpha=0.7)
plt.xlabel("Slova")
plt.ylabel("Četnost")
plt.title("30 nejčastějších slov v datasetu")
plt.xticks(rotation=45)  # Otočení popisků osy X pro lepší čitelnost
plt.grid(axis='y', linestyle='--', alpha=0.7)

# ✅ Zobrazení grafu
plt.show()

import matplotlib.pyplot as plt

# ✅ Výpis historie trénování modelu
history_dict = history.history

# ✅ Získání hodnot loss a accuracy
loss_values = history_dict["loss"]
val_loss_values = history_dict["val_loss"]
accuracy_values = history_dict["accuracy"]
val_accuracy_values = history_dict["val_accuracy"]

epochs = range(1, len(loss_values) + 1)

# ✅ Graf LOSS (ztráta)
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, loss_values, "ro-", label="Training loss")

# Importy knihoven
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from sklearn.impute import SimpleImputer

# Funkce pro načtení a spojení dat
def load_and_merge_data(train_path, test_path, submit_path):
    train_df = pd.read_csv(train_path, sep=';')
    test_df = pd.read_csv(test_path, sep=';')
    submit_df = pd.read_csv(submit_path, sep=';')
    test_df = pd.concat([test_df, submit_df], axis=0, ignore_index=True).drop(columns=["idS"], errors='ignore')
    df_all = pd.concat([train_df, test_df], axis=0, ignore_index=True)
    return df_all

# Funkce pro předzpracování dat
def preprocess_data(df):
    target_column = df.columns[-1]
    X = df.drop(columns=[target_column])
    Y = df[target_column]

    # Imputace chybějících hodnot
    imputer = SimpleImputer(strategy='mean')  # Nebo 'median', 'most_frequent'
    X_imputed = pd.DataFrame(imputer.fit_transform(X.select_dtypes(include=np.number)), columns=X.select_dtypes(include=np.number).columns)

    label_encoder = LabelEncoder()
    Y_encoded = label_encoder.fit_transform(Y)
    one_hot_encoder = OneHotEncoder(sparse_output=False)
    Y_onehot = one_hot_encoder.fit_transform(Y_encoded.reshape(-1, 1))

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_imputed)

    max_features = 10000
    max_len = 20
    if 'title' in df.columns:
        tokenizer = Tokenizer(num_words=max_features, oov_token="<OOV>")
        X_title = df['title'].astype(str).str.lower().fillna("")
        tokenizer.fit_on_texts(X_title)
        sequences = tokenizer.texts_to_sequences(X_title)
        X_title_padded = pad_sequences(sequences, maxlen=max_len, padding="pre")
    else:
        X_title_padded = np.zeros((X.shape[0], max_len))

    X_final = np.hstack([X_scaled, X_title_padded])
    return X_final, Y_onehot

# Funkce pro vytvoření modelu
def create_model(input_shape, num_classes):
    model = Sequential([
        Dense(512, activation='relu', input_shape=(input_shape,), kernel_regularizer=tf.keras.regularizers.l2(0.0005)),
        BatchNormalization(),
        Dropout(0.5),
        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005)),
        BatchNormalization(),
        Dropout(0.4),
        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005)),
        BatchNormalization(),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer=Adam(learning_rate=3e-4), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Načtení a předzpracování dat
df_all = load_and_merge_data('train (2).csv', 'test (1).csv', 'evaluation.csv')
X, Y = preprocess_data(df_all)

# Rozdělení dat
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Vytvoření a trénování modelu
model = create_model(X_train.shape[1], y_train.shape[1])
early_stopping = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, min_lr=1e-6)
history = model.fit(X_train, y_train, epochs=34, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])

# Hodnocení modelu
# ... (viz původní kód)

# Importy knihoven
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from sklearn.impute import SimpleImputer
from sklearn.feature_extraction.text import TfidfVectorizer #Přidaný import

# Funkce pro načtení a spojení dat
def load_and_merge_data(train_path, test_path, submit_path):
    train_df = pd.read_csv(train_path, sep=';')
    test_df = pd.read_csv(test_path, sep=';')
    submit_df = pd.read_csv(submit_path, sep=';')
    test_df = pd.concat([test_df, submit_df], axis=0, ignore_index=True).drop(columns=["idS"], errors='ignore')
    df_all = pd.concat([train_df, test_df], axis=0, ignore_index=True)
    return df_all

# Funkce pro předzpracování dat
def preprocess_data(df):
    target_column = df.columns[-1]
    X = df.drop(columns=[target_column])
    Y = df[target_column]

    # Imputace chybějících hodnot
    imputer = SimpleImputer(strategy='mean')
    X_imputed = pd.DataFrame(imputer.fit_transform(X.select_dtypes(include=np.number)), columns=X.select_dtypes(include=np.number).columns)

    label_encoder = LabelEncoder()
    Y_encoded = label_encoder.fit_transform(Y)
    one_hot_encoder = OneHotEncoder(sparse_output=False)
    Y_onehot = one_hot_encoder.fit_transform(Y_encoded.reshape(-1, 1))

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_imputed)

    if 'title' in df.columns:
        tfidf_vectorizer = TfidfVectorizer(max_features=10000)
        X_title_tfidf = tfidf_vectorizer.fit_transform(df['title'].astype(str).str.lower().fillna("")).toarray()
    else:
        X_title_tfidf = np.zeros((X.shape[0], 10000))

    X_final = np.hstack([X_scaled, X_title_tfidf])
    return X_final, Y_onehot

# Funkce pro vytvoření modelu
def create_model(input_shape, num_classes):
    model = Sequential([
        Dense(512, activation='relu', input_shape=(input_shape,), kernel_regularizer=tf.keras.regularizers.l2(0.0005)),
        BatchNormalization(),
        Dropout(0.5),
        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005)),
        BatchNormalization(),
        Dropout(0.4),
        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005)),
        BatchNormalization(),
        Dropout(0.3),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer=Adam(learning_rate=3e-4), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Načtení a předzpracování dat
df_all = load_and_merge_data('train (2).csv', 'test (1).csv', 'evaluation.csv')
X, Y = preprocess_data(df_all)

# Rozdělení dat
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Vytvoření a trénování modelu
model = create_model(X_train.shape[1], y_train.shape[1])
early_stopping = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, min_lr=1e-6)
history = model.fit(X_train, y_train, epochs=40, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])

# Hodnocení modelu
# ... (viz původní kód)