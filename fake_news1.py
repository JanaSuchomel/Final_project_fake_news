# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1caqXEaF7OLErMoFFj778sGz55fZrFTCG
"""

import pandas as pd

df = pd.read_csv('evaluation.csv', sep=';')
print(df.head())

df = pd.read_csv('test (1).csv', sep=';')
print(df.head())

import pandas as pd
import csv

with open('evaluation.csv', "r", encoding="utf-8") as f:
    delimiter = csv.Sniffer().sniff(f.readline()).delimiter

eval_df = pd.read_csv('evaluation.csv', delimiter=delimiter)

print(f"Detekovaný oddělovač: '{delimiter}'")
print(eval_df.head())

import pandas as pd
import csv

with open('test (1).csv', "r", encoding="utf-8") as f:
    delimiter = csv.Sniffer().sniff(f.readline()).delimiter

test_df = pd.read_csv('test (1).csv', delimiter=delimiter)

print(f"Detekovaný oddělovač: '{delimiter}'")
print(test_df.head())

import pandas as pd
import csv

with open('train (2).csv', "r", encoding="utf-8") as f:
    delimiter = csv.Sniffer().sniff(f.readline()).delimiter

train_df = pd.read_csv('train (2).csv', delimiter=delimiter)

print(f"Detekovaný oddělovač: '{delimiter}'")
print(train_df.head())

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder



# Step 2: Text Preprocessing and Label Encoding
# Here we only need 'title', 'text', and 'label' columns
train_df['content'] = train_df['title'] + " " + train_df['text']  # Combine title and text
eval_df['content'] = eval_df['title'] + " " + eval_df['text']
test_df['content'] = test_df['title'] + " " + test_df['text']

# Encode the labels (1 for fake news, 0 otherwise)
label_encoder = LabelEncoder()
train_df['label'] = label_encoder.fit_transform(train_df['label'])
eval_df['label'] = label_encoder.transform(eval_df['label'])

# Step 3: Train-Test Split
X_train, X_val, y_train, y_val = train_test_split(
    train_df['content'], train_df['label'], test_size=0.2, random_state=42
)

# Step 4: Vectorization using TF-IDF and Model Pipeline
tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')

# Create a pipeline with TF-IDF and Logistic Regression
model_pipeline = Pipeline([
    ('tfidf', tfidf_vectorizer),
    ('clf', LogisticRegression(max_iter=1000))
])

# Step 5: Model Training
model_pipeline.fit(X_train, y_train)

# Step 6: Evaluation on Validation Set
y_pred_val = model_pipeline.predict(X_val)
accuracy = accuracy_score(y_val, y_pred_val)
report = classification_report(y_val, y_pred_val)
conf_matrix = confusion_matrix(y_val, y_pred_val)

print("Validation Accuracy:", accuracy)
print("\nClassification Report:\n", report)
print("\nConfusion Matrix:\n", conf_matrix)

# Step 7: Evaluation on Test Set
if 'label' in test_df.columns:
    y_test = label_encoder.transform(test_df['label'])
    y_pred_test = model_pipeline.predict(test_df['content'])
    test_accuracy = accuracy_score(y_test, y_pred_test)
    test_report = classification_report(y_test, y_pred_test)
    test_conf_matrix = confusion_matrix(y_test, y_pred_test)

    print("Test Accuracy:", test_accuracy)
    print("\nTest Classification Report:\n", test_report)
    print("\nTest Confusion Matrix:\n", test_conf_matrix)
else:
    # If no labels are provided in test data, output predictions
    test_predictions = model_pipeline.predict(test_df['content'])
    test_df['predicted_label'] = label_encoder.inverse_transform(test_predictions)
    test_df.to_csv('/kaggle/working/test_predictions.csv', index=False)
    print("Predictions for test set saved to test_predictions.csv")

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# 📌 1️⃣ **Načtení datasetů**
test_df  = pd.read_csv('test (1).csv', sep=';')
train_df  = pd.read_csv('train (2).csv', sep=';')

# 📌 2️⃣ **Předzpracování dat**
# Předpokládám, že poslední sloupec je cílová proměnná (y)
target_column = train_df.columns[-1]  # Označení cílového sloupce
X_train = train_df.drop(columns=[target_column])
y_train = train_df[target_column]

X_test = test_df.drop(columns=[target_column])
y_test = test_df[target_column]

# 📌 **Použití stejného LabelEncoder pro trénovací i testovací data**
encoder = LabelEncoder()

# Nejdříve naučíme encoder na y_train
encoder.fit(y_train)

# Pak převedeme obě sady dat
y_train = encoder.transform(y_train)
y_test = encoder.transform(y_test)  # Používáme stejný encoder, aby nebyla chyba

# Převod na one-hot encoding (pro neuronovou síť)
y_train = keras.utils.to_categorical(y_train)
y_test = keras.utils.to_categorical(y_test)

# Zpracování textových sloupců
text_columns = X_train.select_dtypes(include=['object']).columns
for col in text_columns:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col])
    X_test[col] = le.transform(X_test[col])

# Normalizace číselných dat
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 📌 3️⃣ **Vytvoření neuronové sítě**
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Vstupní vrstva
    Dropout(0.3),  # Dropout pro snížení přeučení
    Dense(64, activation='relu'),  # Skrytá vrstva
    Dropout(0.3),
    Dense(32, activation='relu'),  # Další skrytá vrstva
    Dense(y_train.shape[1], activation='softmax')  # Výstupní vrstva (softmax pro více tříd)
])

# 📌 4️⃣ **Kompilace modelu**
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# 📌 5️⃣ **Trénování modelu**
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# 📌 6️⃣ **Vyhodnocení modelu**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Převedení pravděpodobností na třídy
y_test_classes = np.argmax(y_test, axis=1)

# Zobrazení klasifikačního reportu
print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes))

# Zobrazení matice záměn
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_classes, y_pred_classes))

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# data sets
test_df = pd.read_csv('test (1).csv', sep=';')
train_df = pd.read_csv('train (2).csv', sep=';')

# processing data
target_column = train_df.columns[-1]  #target column
X_train = train_df.drop(columns=[target_column])
y_train = train_df[target_column]

X_test = test_df.drop(columns=[target_column])
y_test = test_df[target_column]

# use same LabelEncoder for testing and training data
encoder = LabelEncoder()

# learning encoder on y_train
encoder.fit(y_train)

# transform both data sets
y_train = encoder.transform(y_train)
y_test = encoder.transform(y_test)  # Používáme stejný encoder, aby nebyla chyba

# transfer to  one-hot encoding
y_train = keras.utils.to_categorical(y_train)
y_test = keras.utils.to_categorical(y_test)

# processing text columns
text_columns = X_train.select_dtypes(include=['object']).columns
for col in text_columns:
    le = LabelEncoder()
    # Fit the LabelEncoder on the combined unique values from both train and test
    all_values = pd.concat([X_train[col], X_test[col]]).unique()
    le.fit(all_values)
    X_train[col] = le.transform(X_train[col])
    X_test[col] = le.transform(X_test[col])

# normalize numbers columns
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# neural network creation
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer"
    Dropout(0.3),  # Dropout to reduce overfitting
    Dense(64, activation='relu'),  # Hidden layer
    Dropout(0.3),
    Dense(32, activation='relu'),  # Hidden layer
    Dense(y_train.shape[1], activation='softmax')  # Output layer (softmax for multiple classes)
])

# Model compilation
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# training model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# model evaluation
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  #converting probabilities to classes
y_test_classes = np.argmax(y_test, axis=1)

# displaying the classification report
print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes))

# displaying the confusion matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_classes, y_pred_classes))

!pip freeze > requirements.txt

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten
from tensorflow.keras.optimizers import Adam
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_curve, auc

# ✅ Cesta k datům (Uprav dle potřeby)
data_path = "C:\\Users\\fake_news\\"

# ✅ Načtení datasetů
train_df = pd.read_csv(data_path + "train.csv", sep=';')
test_df = pd.read_csv(data_path + "test.csv", sep=';')
submit_df = pd.read_csv(data_path + "submit.csv", sep=';')

# ✅ Spojení testovacích dat
test_df = pd.concat([test_df, submit_df], axis=1).drop("idS", axis=1)
df_all = pd.concat([train_df, test_df])


# ✅ Rozdělení na vstupní (X) a výstupní (Y) data
target_column = df_all.columns[-1]
X = df_all.drop(columns=[target_column])
Y = df_all[target_column]

# ✅ Label Encoding pro výstupní data
encoder = LabelEncoder()
Y = encoder.fit_transform(Y)
Y = keras.utils.to_categorical(Y)  # Převod na one-hot encoding

# ✅ Zpracování textových sloupců
text_columns = X.select_dtypes(include=['object']).columns
for col in text_columns:
    le = LabelEncoder()
    all_values = pd.concat([X[col], X[col]]).unique()  # Sloučení hodnot z trénovacích a testovacích dat
    le.fit(all_values)
    X[col] = le.transform(X[col])

# ✅ Normalizace číselných sloupců
scaler = StandardScaler()
X = scaler.fit_transform(X)

# ✅ Tokenizace textových dat (pokud je sloupec `title`)
if 'title' in df_all.columns:
    max_features = 5000
    max_len = 40
    tokenizer = Tokenizer(num_words=max_features)
    X_title = df_all['title'].astype(str).fillna("")
    tokenizer.fit_on_texts(X_title)
    sequences = tokenizer.texts_to_sequences(X_title)
    X_title = pad_sequences(sequences, maxlen=max_len, padding="pre")
else:
    X_title = None

# ✅ Rozdělení na trénovací a testovací sady
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ✅ Vytvoření neuronové sítě
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Vstupní vrstva
    Dropout(0.3),  # Dropout pro snížení přeučení
    Dense(64, activation='relu'),  # Skrytá vrstva
    Dropout(0.3),
    Dense(32, activation='relu'),  # Skrytá vrstva
    Dense(y_train.shape[1], activation='softmax')  # Výstupní vrstva (softmax pro více tříd)
])

# ✅ Kompilace modelu
model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ✅ Trénování modelu
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# ✅ Zobrazení průběhu trénování
history_dict = history.history
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history_dict["loss"], "ro", label="Training loss")
plt.plot(history_dict["val_loss"], "r", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(history_dict["accuracy"], "bo", label="Training accuracy")
plt.plot(history_dict["val_accuracy"], "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()

plt.show()

# ✅ Predikce na testovacích datech
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Převod pravděpodobností na třídy
y_test_classes = np.argmax(y_test, axis=1)

# ✅ Zobrazení klasifikačního reportu
print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes))

# ✅ Zobrazení matice záměn
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_classes, y_pred_classes))

# ✅ ROC křivka a AUC skóre
y_pred_proba = y_pred.ravel()
fpr, tpr, thresholds = roc_curve(y_test_classes, y_pred_proba)
auc_score = auc(fpr, tpr)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f'ROC (AUC = {auc_score:.3f})')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC Curve")
plt.legend()
plt.grid()
plt.show()

# ✅ Vyhodnocení modelu
results = model.evaluate(X_test, y_test)
print(f"\nModel Loss: {results[0]}, Accuracy: {results[1]}")