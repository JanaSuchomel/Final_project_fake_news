# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1caqXEaF7OLErMoFFj778sGz55fZrFTCG
"""

import pandas as pd

df = pd.read_csv('evaluation.csv', sep=';')
print(df.head())

df = pd.read_csv('test (1).csv', sep=';')
print(df.head())

import pandas as pd
import csv

with open('evaluation.csv', "r", encoding="utf-8") as f:
    delimiter = csv.Sniffer().sniff(f.readline()).delimiter

eval_df = pd.read_csv('evaluation.csv', delimiter=delimiter)

print(f"Detekovan√Ω oddƒõlovaƒç: '{delimiter}'")
print(eval_df.head())

import pandas as pd
import csv

with open('test (1).csv', "r", encoding="utf-8") as f:
    delimiter = csv.Sniffer().sniff(f.readline()).delimiter

test_df = pd.read_csv('test (1).csv', delimiter=delimiter)

print(f"Detekovan√Ω oddƒõlovaƒç: '{delimiter}'")
print(test_df.head())

import pandas as pd
import csv

with open('train (2).csv', "r", encoding="utf-8") as f:
    delimiter = csv.Sniffer().sniff(f.readline()).delimiter

train_df = pd.read_csv('train (2).csv', delimiter=delimiter)

print(f"Detekovan√Ω oddƒõlovaƒç: '{delimiter}'")
print(train_df.head())

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder



# Step 2: Text Preprocessing and Label Encoding
# Here we only need 'title', 'text', and 'label' columns
train_df['content'] = train_df['title'] + " " + train_df['text']  # Combine title and text
eval_df['content'] = eval_df['title'] + " " + eval_df['text']
test_df['content'] = test_df['title'] + " " + test_df['text']

# Encode the labels (1 for fake news, 0 otherwise)
label_encoder = LabelEncoder()
train_df['label'] = label_encoder.fit_transform(train_df['label'])
eval_df['label'] = label_encoder.transform(eval_df['label'])

# Step 3: Train-Test Split
X_train, X_val, y_train, y_val = train_test_split(
    train_df['content'], train_df['label'], test_size=0.2, random_state=42
)

# Step 4: Vectorization using TF-IDF and Model Pipeline
tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')

# Create a pipeline with TF-IDF and Logistic Regression
model_pipeline = Pipeline([
    ('tfidf', tfidf_vectorizer),
    ('clf', LogisticRegression(max_iter=1000))
])

# Step 5: Model Training
model_pipeline.fit(X_train, y_train)

# Step 6: Evaluation on Validation Set
y_pred_val = model_pipeline.predict(X_val)
accuracy = accuracy_score(y_val, y_pred_val)
report = classification_report(y_val, y_pred_val)
conf_matrix = confusion_matrix(y_val, y_pred_val)

print("Validation Accuracy:", accuracy)
print("\nClassification Report:\n", report)
print("\nConfusion Matrix:\n", conf_matrix)

# Step 7: Evaluation on Test Set
if 'label' in test_df.columns:
    y_test = label_encoder.transform(test_df['label'])
    y_pred_test = model_pipeline.predict(test_df['content'])
    test_accuracy = accuracy_score(y_test, y_pred_test)
    test_report = classification_report(y_test, y_pred_test)
    test_conf_matrix = confusion_matrix(y_test, y_pred_test)

    print("Test Accuracy:", test_accuracy)
    print("\nTest Classification Report:\n", test_report)
    print("\nTest Confusion Matrix:\n", test_conf_matrix)
else:
    # If no labels are provided in test data, output predictions
    test_predictions = model_pipeline.predict(test_df['content'])
    test_df['predicted_label'] = label_encoder.inverse_transform(test_predictions)
    test_df.to_csv('/kaggle/working/test_predictions.csv', index=False)
    print("Predictions for test set saved to test_predictions.csv")

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# üìå 1Ô∏è‚É£ **Naƒçten√≠ dataset≈Ø**
test_df  = pd.read_csv('test (1).csv', sep=';')
train_df  = pd.read_csv('train (2).csv', sep=';')

# üìå 2Ô∏è‚É£ **P≈ôedzpracov√°n√≠ dat**
# P≈ôedpokl√°d√°m, ≈æe posledn√≠ sloupec je c√≠lov√° promƒõnn√° (y)
target_column = train_df.columns[-1]  # Oznaƒçen√≠ c√≠lov√©ho sloupce
X_train = train_df.drop(columns=[target_column])
y_train = train_df[target_column]

X_test = test_df.drop(columns=[target_column])
y_test = test_df[target_column]

# üìå **Pou≈æit√≠ stejn√©ho LabelEncoder pro tr√©novac√≠ i testovac√≠ data**
encoder = LabelEncoder()

# Nejd≈ô√≠ve nauƒç√≠me encoder na y_train
encoder.fit(y_train)

# Pak p≈ôevedeme obƒõ sady dat
y_train = encoder.transform(y_train)
y_test = encoder.transform(y_test)  # Pou≈æ√≠v√°me stejn√Ω encoder, aby nebyla chyba

# P≈ôevod na one-hot encoding (pro neuronovou s√≠≈•)
y_train = keras.utils.to_categorical(y_train)
y_test = keras.utils.to_categorical(y_test)

# Zpracov√°n√≠ textov√Ωch sloupc≈Ø
text_columns = X_train.select_dtypes(include=['object']).columns
for col in text_columns:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col])
    X_test[col] = le.transform(X_test[col])

# Normalizace ƒç√≠seln√Ωch dat
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# üìå 3Ô∏è‚É£ **Vytvo≈ôen√≠ neuronov√© s√≠tƒõ**
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Vstupn√≠ vrstva
    Dropout(0.3),  # Dropout pro sn√≠≈æen√≠ p≈ôeuƒçen√≠
    Dense(64, activation='relu'),  # Skryt√° vrstva
    Dropout(0.3),
    Dense(32, activation='relu'),  # Dal≈°√≠ skryt√° vrstva
    Dense(y_train.shape[1], activation='softmax')  # V√Ωstupn√≠ vrstva (softmax pro v√≠ce t≈ô√≠d)
])

# üìå 4Ô∏è‚É£ **Kompilace modelu**
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# üìå 5Ô∏è‚É£ **Tr√©nov√°n√≠ modelu**
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# üìå 6Ô∏è‚É£ **Vyhodnocen√≠ modelu**
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # P≈ôeveden√≠ pravdƒõpodobnost√≠ na t≈ô√≠dy
y_test_classes = np.argmax(y_test, axis=1)

# Zobrazen√≠ klasifikaƒçn√≠ho reportu
print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes))

# Zobrazen√≠ matice z√°mƒõn
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_classes, y_pred_classes))

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# data sets
test_df = pd.read_csv('test (1).csv', sep=';')
train_df = pd.read_csv('train (2).csv', sep=';')

# processing data
target_column = train_df.columns[-1]  #target column
X_train = train_df.drop(columns=[target_column])
y_train = train_df[target_column]

X_test = test_df.drop(columns=[target_column])
y_test = test_df[target_column]

# use same LabelEncoder for testing and training data
encoder = LabelEncoder()

# learning encoder on y_train
encoder.fit(y_train)

# transform both data sets
y_train = encoder.transform(y_train)
y_test = encoder.transform(y_test)  # Pou≈æ√≠v√°me stejn√Ω encoder, aby nebyla chyba

# transfer to  one-hot encoding
y_train = keras.utils.to_categorical(y_train)
y_test = keras.utils.to_categorical(y_test)

# processing text columns
text_columns = X_train.select_dtypes(include=['object']).columns
for col in text_columns:
    le = LabelEncoder()
    # Fit the LabelEncoder on the combined unique values from both train and test
    all_values = pd.concat([X_train[col], X_test[col]]).unique()
    le.fit(all_values)
    X_train[col] = le.transform(X_train[col])
    X_test[col] = le.transform(X_test[col])

# normalize numbers columns
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# neural network creation
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer"
    Dropout(0.3),  # Dropout to reduce overfitting
    Dense(64, activation='relu'),  # Hidden layer
    Dropout(0.3),
    Dense(32, activation='relu'),  # Hidden layer
    Dense(y_train.shape[1], activation='softmax')  # Output layer (softmax for multiple classes)
])

# Model compilation
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# training model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# model evaluation
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  #converting probabilities to classes
y_test_classes = np.argmax(y_test, axis=1)

# displaying the classification report
print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes))

# displaying the confusion matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_classes, y_pred_classes))

!pip freeze > requirements.txt

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten
from tensorflow.keras.optimizers import Adam
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_curve, auc

# ‚úÖ Cesta k dat≈Øm (Uprav dle pot≈ôeby)
data_path = "C:\\Users\\fake_news\\"

# ‚úÖ Naƒçten√≠ dataset≈Ø
train_df = pd.read_csv(data_path + "train.csv", sep=';')
test_df = pd.read_csv(data_path + "test.csv", sep=';')
submit_df = pd.read_csv(data_path + "submit.csv", sep=';')

# ‚úÖ Spojen√≠ testovac√≠ch dat
test_df = pd.concat([test_df, submit_df], axis=1).drop("idS", axis=1)
df_all = pd.concat([train_df, test_df])


# ‚úÖ Rozdƒõlen√≠ na vstupn√≠ (X) a v√Ωstupn√≠ (Y) data
target_column = df_all.columns[-1]
X = df_all.drop(columns=[target_column])
Y = df_all[target_column]

# ‚úÖ Label Encoding pro v√Ωstupn√≠ data
encoder = LabelEncoder()
Y = encoder.fit_transform(Y)
Y = keras.utils.to_categorical(Y)  # P≈ôevod na one-hot encoding

# ‚úÖ Zpracov√°n√≠ textov√Ωch sloupc≈Ø
text_columns = X.select_dtypes(include=['object']).columns
for col in text_columns:
    le = LabelEncoder()
    all_values = pd.concat([X[col], X[col]]).unique()  # Slouƒçen√≠ hodnot z tr√©novac√≠ch a testovac√≠ch dat
    le.fit(all_values)
    X[col] = le.transform(X[col])

# ‚úÖ Normalizace ƒç√≠seln√Ωch sloupc≈Ø
scaler = StandardScaler()
X = scaler.fit_transform(X)

# ‚úÖ Tokenizace textov√Ωch dat (pokud je sloupec `title`)
if 'title' in df_all.columns:
    max_features = 5000
    max_len = 40
    tokenizer = Tokenizer(num_words=max_features)
    X_title = df_all['title'].astype(str).fillna("")
    tokenizer.fit_on_texts(X_title)
    sequences = tokenizer.texts_to_sequences(X_title)
    X_title = pad_sequences(sequences, maxlen=max_len, padding="pre")
else:
    X_title = None

# ‚úÖ Rozdƒõlen√≠ na tr√©novac√≠ a testovac√≠ sady
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ‚úÖ Vytvo≈ôen√≠ neuronov√© s√≠tƒõ
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Vstupn√≠ vrstva
    Dropout(0.3),  # Dropout pro sn√≠≈æen√≠ p≈ôeuƒçen√≠
    Dense(64, activation='relu'),  # Skryt√° vrstva
    Dropout(0.3),
    Dense(32, activation='relu'),  # Skryt√° vrstva
    Dense(y_train.shape[1], activation='softmax')  # V√Ωstupn√≠ vrstva (softmax pro v√≠ce t≈ô√≠d)
])

# ‚úÖ Kompilace modelu
model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ‚úÖ Tr√©nov√°n√≠ modelu
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# ‚úÖ Zobrazen√≠ pr≈Øbƒõhu tr√©nov√°n√≠
history_dict = history.history
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history_dict["loss"], "ro", label="Training loss")
plt.plot(history_dict["val_loss"], "r", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(history_dict["accuracy"], "bo", label="Training accuracy")
plt.plot(history_dict["val_accuracy"], "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid()

plt.show()

# ‚úÖ Predikce na testovac√≠ch datech
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # P≈ôevod pravdƒõpodobnost√≠ na t≈ô√≠dy
y_test_classes = np.argmax(y_test, axis=1)

# ‚úÖ Zobrazen√≠ klasifikaƒçn√≠ho reportu
print("\nClassification Report:")
print(classification_report(y_test_classes, y_pred_classes))

# ‚úÖ Zobrazen√≠ matice z√°mƒõn
print("\nConfusion Matrix:")
print(confusion_matrix(y_test_classes, y_pred_classes))

# ‚úÖ ROC k≈ôivka a AUC sk√≥re
y_pred_proba = y_pred.ravel()
fpr, tpr, thresholds = roc_curve(y_test_classes, y_pred_proba)
auc_score = auc(fpr, tpr)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f'ROC (AUC = {auc_score:.3f})')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("ROC Curve")
plt.legend()
plt.grid()
plt.show()

# ‚úÖ Vyhodnocen√≠ modelu
results = model.evaluate(X_test, y_test)
print(f"\nModel Loss: {results[0]}, Accuracy: {results[1]}")